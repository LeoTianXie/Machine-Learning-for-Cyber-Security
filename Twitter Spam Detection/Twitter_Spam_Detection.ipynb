{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCa0_XoXj40Q"
      },
      "source": [
        "# Twitter Spam Detection\n",
        "\n",
        "## Feature Extraction\n",
        "\n",
        "We will begin our ML pipeline with feature extraction in this homework, as the dataset is good enough and does not need any special pre-processing. As usual, we will load the dataset into memory, extract samples features into vectors and split the dataset into training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mpUDnGbSj40R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "7fb5414c-419b-4b53-83c7-88b2e37bb905"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label\n",
              "0  @baydiangirl it definitely moved some church f...      0\n",
              "1  @suebob Looks like #TXlege made up new rules f...      0\n",
              "2  RT @saleemchat: #WJChat #A1B 5 yrs: People ski...      0\n",
              "3                            @SajidaBalouch Huhhhhh       0\n",
              "4              @Dukester_94 im the lad of all bibles      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-55879309-a2af-4df7-a385-c9dbffad081c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@baydiangirl it definitely moved some church f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@suebob Looks like #TXlege made up new rules f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RT @saleemchat: #WJChat #A1B 5 yrs: People ski...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@SajidaBalouch Huhhhhh</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@Dukester_94 im the lad of all bibles</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-55879309-a2af-4df7-a385-c9dbffad081c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-55879309-a2af-4df7-a385-c9dbffad081c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-55879309-a2af-4df7-a385-c9dbffad081c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 45587,\n        \"samples\": [\n          \"RT @MyQueenTroian: Troian is my favorite. I love Troian. http://t.co/UaD55Eknjw\",\n          \"@Vijaynarain yup, we're never nsync\",\n          \"What Is An Acceptable Start-Up Term Sheet These Days? http://bit.ly/x1mM5 RT @jlojlo @Visionscaper @rainierco #fb\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "## [ TODO ]\n",
        "# 1) Load all data from `twitter-mini.csv` into variable `data`\n",
        "# 2) Preview first few samples\n",
        "data = pd.read_csv(\"twitter-mini.csv\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t21bJaUzj40S"
      },
      "source": [
        "In lab 2 we have learned how to use the `CountVectorizer` feature extractor, which counts the occurance of each token (letter here) in a sample (Twitter message text here). This time, we will also try `TfidfVectorizer` and see which feature extractor performs better. Here we will introduce the algorithm it implemented, **TF-IDF**:\n",
        "\n",
        "> **TF-IDF** is the abbreviation for **term frequencyâ€“inverse document frequency**, and is defined as the product of the two frequency. The most classical version of TF-IDF is:\n",
        ">\n",
        "> $$\n",
        "tf-idf(t) = tf(t, d) \\cdot idf(t, D) = \\frac{f_{t,d}}{|d|} \\log \\frac{|D|}{|D_t|}\n",
        "$$\n",
        ">\n",
        "> Where $t$ is the term, $D$ is the collection of all documents, $d$ is one of the document that contains $t$ and $D_t$ is the subset of documents that contains term $t$. The advantage of TF-IDF is that frequent and useless words, such as \"the\", \"a\" and \"an\" will receive lower weight, since they appear in almost all documents. On the other hand, unique yet frequent (in any $d \\in D_t$) words will get higher weight. Research has shown that TF-IDF is easy to implement and performs relatively well.\n",
        "\n",
        "Before we run these feature extractors on our dataset, we need to convert each message into a sequence of terms. Since Twitter messages often contain emojis, abbreviations, hashtags and other composition of characters, we can't use the word-level analyzer as we have done in lab 2. Instead, we will generate **character N-grams** from these messages.\n",
        "\n",
        "> A **character N-gram** consists of N consecutive characters. Here we show all character 1, 2, 3 and 4-grams for word \"cold\":\n",
        "> <img src=\"attachment:char-ngram.jpg\" width=\"50%\" />\n",
        "\n",
        "Both `CountVectorizer` and `TfidfVectorizer` provide support for character N-grams. Refer to `sklearn`'s documents to build a character N-gram analyzer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dxByYcJ6j40S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18680293-189e-4182-89ea-9cb44882bedd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting features with `CountVectorizer`...\n",
            "Extracting features with `TfidfVectorizer`...\n",
            "Completed.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "texts = data[\"text\"].values\n",
        "# 1) Extract features of all messages into `feat1` with `CountVectorizer` using CHARACTER trigrams\n",
        "# 2) Repeat with `TfidfVectorizer` and save generated features as `feat2`\n",
        "print(\"Extracting features with `CountVectorizer`...\")\n",
        "cv = CountVectorizer(analyzer=\"char\", ngram_range=(3, 3))\n",
        "feat1 = cv.fit_transform(texts)\n",
        "\n",
        "print(\"Extracting features with `TfidfVectorizer`...\")\n",
        "tfidf = TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 3))\n",
        "feat2 = tfidf.fit_transform(texts)\n",
        "\n",
        "print(\"Completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7FRYR00j40T"
      },
      "source": [
        "We will also try combining two kinds of feature generated above into a single feature. We will use `scipy.sparse.hstack` to concatenate corresonding features for each sample. Remember, concatenated features matrix should have same amount of samples as `feat1` and `feat2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5QjerxF7j40T"
      },
      "outputs": [],
      "source": [
        "# Concatenate token count features and TF-IDF features on the feature dimension,\n",
        "# name the combined feature matrix `feat_comb`.\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "feat_comb = hstack([feat1, feat2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5hMXCezj40T"
      },
      "source": [
        "Finally, let's take a look at the labels. In our dataset, 0 represents messages from real Twitter users and 1 represents spam messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "trteOuMnj40T"
      },
      "outputs": [],
      "source": [
        "y = data[\"label\"].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU8JwW02j40T"
      },
      "source": [
        "## Training\n",
        "\n",
        "As usual, we will start by doing a 80% / 20% training-test split. Like lab 2, we suggest you to **use the same random seed for all splits** for comparable and reproducible results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NdvLp3Z1j40U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "808319a7-08d3-4f0a-f0da-8c25c48af639"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spliting token count features...\n",
            "Spliting TF-IDF features...\n",
            "Spliting combined features...\n",
            "Completed.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# 1) Split token count features into 80/20 training and test sets\n",
        "# 2) Split TF-IDF features into 80/20 training and test sets\n",
        "# 3) Split combined features into 80/20 training and test sets\n",
        "\n",
        "# Token count features\n",
        "print(\"Spliting token count features...\")\n",
        "X1_train, X1_test, y_train, y_test = train_test_split(\n",
        "    feat1, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# TF-IDF features\n",
        "print(\"Spliting TF-IDF features...\")\n",
        "X2_train, X2_test, _, _ = train_test_split(\n",
        "    feat2, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Combined features\n",
        "print(\"Spliting combined features...\")\n",
        "Xc_train, Xc_test, _, _ = train_test_split(\n",
        "    feat_comb, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgGEBXFtj40U"
      },
      "source": [
        "We are now ready to train **binary classifiers** for spam message detection. Apart from the `LogisticRegression` classifier you have already seen and used during Lab 1, we will try a few more classifiers:\n",
        "\n",
        "* `MultinomialNB` (Naive Bayes):  \n",
        "  This classifier is based on the **Naive Bayes** assumption that, *given the class label*, individual features are conditionally independent.  \n",
        "  For text classification, `MultinomialNB` is used with **token count** or **TF-IDF-like** features.\n",
        "\n",
        "  It models the probability of a message belonging to a class $ y \\in \\{\\text{spam}, \\text{ham}\\} $ as:\n",
        "\n",
        "  $$\n",
        "  P(y \\mid \\mathbf{x}) \\propto P(y)\\prod_{j=1}^{d} P(x_j \\mid y)\n",
        "   $$\n",
        "\n",
        "  where $\\mathbf{x} = (x_1, x_2, \\dots, x_d)$ is the feature vector (e.g., word counts), and $P(x_j \\mid y)$ is learned from training data.  \n",
        "  The predicted label is chosen using:\n",
        "\n",
        "   $$\n",
        "  y_{pred} = \\arg\\max_y \\; P(y \\mid \\mathbf{x})\n",
        "   $$\n",
        "\n",
        "  In practice, the computation is done in the **log domain** to avoid numerical underflow:\n",
        "\n",
        "  $$\n",
        "  y_{pred} = \\arg\\max_y \\left( \\log P(y) + \\sum_{j=1}^{d} x_j \\log P(x_j \\mid y) \\right)\n",
        "  $$\n",
        "\n",
        "* `LinearSVC`:  \n",
        "  This classifier is based on **linear SVM**, a popular algorithm for binary classification. The linear SVM algorithm treats the feature vector of each sample as a point in a linear space, and tries to find a hyperplane that separates points of the two classes as well as possible.  \n",
        "\n",
        "  In practice, we optimize a **squared hinge loss with regularization**, since the features may not be perfectly linearly separable:\n",
        "\n",
        "   $$\n",
        "  l_{hinge} = \\max(0, y(1 - \\mathbf{x}^T \\mathbf{w}))^2 + \\beta ||\\mathbf{w}||_2^2\n",
        "   $$\n",
        "\n",
        "  Similarly, the classification result is determined by:\n",
        "\n",
        "  $$\n",
        "  y_{pred} = sign(1 - \\mathbf{x}^T \\mathbf{w})\n",
        " $$\n",
        "\n",
        "We have three kinds of features (**token count**, **TF-IDF**, and **combined**) and three kinds of classifiers. Here we will only try three combinations and compare their performance:\n",
        "\n",
        "* `MultinomialNB` on token count features  \n",
        "* `LogisticRegression` on TF-IDF features  \n",
        "  - Try increasing the number of iterations if a warning of non-convergence pops up  \n",
        "* `LinearSVC` on combined features  \n",
        "  - For linear SVM, you can ignore the non-convergence warning as it does not affect performance  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "TdeGmMHYj40U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5db32f2a-ef92-43c0-8d96-33eda8d113be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Naive Bayes classifier on token count training set...\n",
            "Training logistic regression classifier on TF-IDF training set...\n",
            "Training SVM classifier on combined features training set...\n",
            "Completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "# 1) Train a `MultinomialNB` classifier called `model1` on token count training set\n",
        "# 2) Train a `LogisticRegression` classifier called `model2` on TF-IDF training set\n",
        "# 3) Train a `LinearSVC` classifier called `model_comb` on combined features training set\n",
        "\n",
        "print(\"Training Naive Bayes classifier on token count training set...\")\n",
        "model1 = MultinomialNB()\n",
        "model1.fit(X1_train, y_train)\n",
        "\n",
        "print(\"Training logistic regression classifier on TF-IDF training set...\")\n",
        "model2 = LogisticRegression(max_iter=1000)\n",
        "model2.fit(X2_train, y_train)\n",
        "\n",
        "print(\"Training SVM classifier on combined features training set...\")\n",
        "model_comb = LinearSVC()\n",
        "model_comb.fit(Xc_train, y_train)\n",
        "\n",
        "print(\"Completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHZYU83qj40U"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Finally, it's time to evaluate our models. You probably noticed that we have repeated similar code for different feature extraction and classification models. To avoid that, we will use `for` loops to make evaluation simpler:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Ovl_tiKmj40U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2491f55d-7b21-4f4b-d26e-56691d6f61d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================\n",
            "1. Naive Bayes on Token count:\n",
            "\n",
            "Training metrics:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.99      0.95     19978\n",
            "           1       0.98      0.91      0.95     20022\n",
            "\n",
            "    accuracy                           0.95     40000\n",
            "   macro avg       0.95      0.95      0.95     40000\n",
            "weighted avg       0.95      0.95      0.95     40000\n",
            "\n",
            "Training confusion matrix:\n",
            " [[19694   284]\n",
            " [ 1748 18274]] \n",
            "\n",
            "Train accuracy: 0.9492 \n",
            "\n",
            "Test metrics:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95      5022\n",
            "           1       0.98      0.91      0.94      4978\n",
            "\n",
            "    accuracy                           0.95     10000\n",
            "   macro avg       0.95      0.95      0.95     10000\n",
            "weighted avg       0.95      0.95      0.95     10000\n",
            "\n",
            "Test Confusion Matrix:\n",
            " [[4939   83]\n",
            " [ 456 4522]] \n",
            "\n",
            "Test Accuracy: 0.9461\n",
            "====================\n",
            "\n",
            "====================\n",
            "2. Logistic on TF-IDF:\n",
            "\n",
            "Training metrics:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.99      0.97     19978\n",
            "           1       0.99      0.94      0.96     20022\n",
            "\n",
            "    accuracy                           0.97     40000\n",
            "   macro avg       0.97      0.97      0.97     40000\n",
            "weighted avg       0.97      0.97      0.97     40000\n",
            "\n",
            "Training confusion matrix:\n",
            " [[19822   156]\n",
            " [ 1228 18794]] \n",
            "\n",
            "Train accuracy: 0.9654 \n",
            "\n",
            "Test metrics:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.99      0.96      5022\n",
            "           1       0.99      0.93      0.96      4978\n",
            "\n",
            "    accuracy                           0.96     10000\n",
            "   macro avg       0.96      0.96      0.96     10000\n",
            "weighted avg       0.96      0.96      0.96     10000\n",
            "\n",
            "Test Confusion Matrix:\n",
            " [[4957   65]\n",
            " [ 366 4612]] \n",
            "\n",
            "Test Accuracy: 0.9569\n",
            "====================\n",
            "\n",
            "====================\n",
            "3. SVM on Combined:\n",
            "\n",
            "Training metrics:               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     19978\n",
            "           1       1.00      1.00      1.00     20022\n",
            "\n",
            "    accuracy                           1.00     40000\n",
            "   macro avg       1.00      1.00      1.00     40000\n",
            "weighted avg       1.00      1.00      1.00     40000\n",
            "\n",
            "Training confusion matrix:\n",
            " [[19978     0]\n",
            " [   28 19994]] \n",
            "\n",
            "Train accuracy: 0.9993 \n",
            "\n",
            "Test metrics:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.95      0.95      5022\n",
            "           1       0.95      0.95      0.95      4978\n",
            "\n",
            "    accuracy                           0.95     10000\n",
            "   macro avg       0.95      0.95      0.95     10000\n",
            "weighted avg       0.95      0.95      0.95     10000\n",
            "\n",
            "Test Confusion Matrix:\n",
            " [[4767  255]\n",
            " [ 257 4721]] \n",
            "\n",
            "Test Accuracy: 0.9488\n",
            "====================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "features = [(\"Token count\", feat1), (\"TF-IDF\", feat2), (\"Combined\", feat_comb)]\n",
        "models = [(\"Naive Bayes\", model1), (\"Logistic\", model2), (\"SVM\", model_comb)]\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "# Complete missing code in the loop body\n",
        "for i, ((feat_name, feat), (model_name, model)) in enumerate(zip(features, models)):\n",
        "    # Begin of metrics for combination\n",
        "    print(\"====================\")\n",
        "    print(f\"{i+1}. {model_name} on {feat_name}:\\n\")\n",
        "\n",
        "    # 1) Split features into training and test set\n",
        "    # (Note: you should use the same random number seed as the one in the training-test split above)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        feat, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # 2) Predict labels for training data\n",
        "    pred_train = model.predict(X_train)\n",
        "\n",
        "    # 3) Print metrics for training set (precision, recall and F1 metrics)\n",
        "    print(\"Training metrics:\", classification_report(y_train, pred_train))\n",
        "    # Confusion matrix\n",
        "    print(\"Training confusion matrix:\\n\", confusion_matrix(y_train, pred_train), \"\\n\")\n",
        "    # Accuracy\n",
        "    print(\"Train accuracy:\", accuracy_score(y_train, pred_train), \"\\n\")\n",
        "\n",
        "    # 4) Predict labels for test data\n",
        "    pred_test = model.predict(X_test)\n",
        "\n",
        "    # 5) Print metrics for test set (precision, recall and F1 metrics)\n",
        "    print(\"Test metrics:\", classification_report(y_test, pred_test))\n",
        "    # Confusion matrix\n",
        "    print(\"Test Confusion Matrix:\\n\", confusion_matrix(y_test, pred_test), \"\\n\")\n",
        "    # Accuracy\n",
        "    print(\"Test Accuracy:\", accuracy_score(y_test, pred_test))\n",
        "\n",
        "    # End of metrics for combination\n",
        "    print(\"====================\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zydHWRHwj40V"
      },
      "source": [
        "# Questions\n",
        "Please answer the following questions. You can just write down the answer below each question in this cell.\n",
        "\n",
        "\n",
        "1. Please compute `tf-idf` of word-level **bigrams** for the following three messages **by hand**. You should use the TF-IDF definition introduced in class.\n",
        "\n",
        "  * Message 1: \"vegetables are good for health\"\n",
        "  * Message 2: \"fruits are good source of vitamins\"\n",
        "  * Message 3: \"proteins are good for health\"\n",
        "\n",
        "  Compute and sort intermediate and final results for **all occurring bigrams in the following order**:\n",
        "  \n",
        "  ```\n",
        "  [\"vegetables are\", \"are good\", \"good for\", \"for health\", \"fruits are\", \"good source\", \"source of\", \"of vitamins\", \"proteins are\"]\n",
        "  ```\n",
        "  \n",
        "  Then complete the following questions:\n",
        "\n",
        "  1. Term Frequency (`tf`) of **bigrams for message 2**.\n",
        "  \n",
        "  *[0, 1, 0, 0, 1, 1, 1, 1, 0]*\n",
        "\n",
        "  2. Inverse Document Frequency (`idf`) of **bigrams for message 2**.\n",
        "    \n",
        "  *[log3, 0, log(3/2), log(3/2), log3, log3, log3, log3, log3]*\n",
        "\n",
        "  3. `tf-idf` of **bigrams for message 2**.\n",
        "    \n",
        "  *[0,0,0,0,log3,log3,log3,log3,0]*\n",
        "\n",
        "  4. Is the `tf-idf` value for the term \"are good\" equal to 0? If it's 0, can you explain why it makes sense to have 0 for that term?\n",
        "    \n",
        "  *Yes it is zero. This make sense because it appears in all the documents, making it useless in contributing to the distinguishing power.*\n",
        "\n",
        "  5. If we consider unigram (1-gram) based bag of words, how many words in these three messages would be assigned a 0 value tf-idf? And please explain why.\n",
        "    \n",
        "  *\"Are\" and \"Good\" are going to be assigned a 0 value, because they appeared in every single sentence, making them useless in distinguishing the sentences apart from a statistical standpoint.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9IBYCktj40V"
      },
      "source": [
        "\n",
        "2. What does `support` mean in the `classification_report`?\n",
        "\n",
        "*Support is the number of true samples for each class in the dataset being evaluated.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1rTOk1aj40V"
      },
      "source": [
        "3. If the training data is highly unbalanced, is `accuracy` a good metric for the classifiers' performance? Does a high accuracy necessarily means a high f1-score? Compare `accuracy` with `f1-score`. Based on your criterion, which type of classifier is better? Please give some justifications to support your answer.\n",
        "\n",
        "*No, when data is imbalanced, accuracy is not a good metric for the classifiers' performance. A high accuracy score does not translate to a high F1 score. Say for example with a dataset with 95 correct and 5 wrong, and the model predicted 100 to be correct. This scenario makes accuracy 95%, which seems pretty good, but the recall for the wrong is 0, and so the F1 score would be 0 too, which means this model is not predicting well.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40Gi_np_j40V"
      },
      "source": [
        "4. For most cases, the amount of bad data is much less than good data. How can you mitigate the data imbalance issue?\n",
        "\n",
        "*Two ways to do this: reducing the amount of good data (undersampling), or sythesize the bad data (oversampling).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPMnnVRtj40V"
      },
      "source": [
        "## References\n",
        "\n",
        "1. CRESCI-2017 Dataset: https://botometer.osome.iu.edu/bot-repository/datasets.html\n",
        "2. TF-IDF: https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
        "3. N-grams: https://en.wikipedia.org/wiki/N-gram\n",
        "4. `scikit-learn` documentation: https://scikit-learn.org/stable/modules/classes.html\n",
        "5. Ridge regression: https://en.wikipedia.org/wiki/Tikhonov_regularization\n",
        "6. SVM: https://en.wikipedia.org/wiki/Support-vector_machine\n",
        "7. Lasso: https://en.wikipedia.org/wiki/Lasso_(statistics)\n",
        "8. ElasticNet: https://en.wikipedia.org/wiki/Elastic_net_regularization\n",
        "9. Kernel method: https://en.wikipedia.org/wiki/Kernel_method"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}